{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Gym\n",
    "*If code differs between the notebook and the curriculum, go with the notebook!!!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space.n\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStateSize():\n",
    "    state=env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    obs, _, _, _ = env.step(action)\n",
    "    return len(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "state_space = getStateSize()\n",
    "print(state_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random games test\n",
    "We did this in the previous lesson, so skip it if you wish. We'll see it played later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_random_games_first():\n",
    "    for episode in range(5):\n",
    "        env.reset()\n",
    "        for t in range(500):\n",
    "            env.render()\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                env.close()\n",
    "                break\n",
    "\n",
    "# uncomment line below to watch the random agent play\n",
    "# some_random_games_first()                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(action_space, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is crucially important! Mind the details!\n",
    "#\n",
    "def initial_data(number_of_games, game_turns, acceptable_score):\n",
    "    # lists for features X and labels y\n",
    "    # i.e., states of the environment \n",
    "    # (observations) are in X and the \n",
    "    # proposed actions to take are in y\n",
    "    X = []\n",
    "    y = []\n",
    "    # one hot encoded vector for actions.\n",
    "    # initialized empty\n",
    "    one_hot = [0 for i in range(action_space)]\n",
    "    # How many games should we play? A \n",
    "    # good place to start is to balance\n",
    "    # number of games and acceptable score\n",
    "    # so that you're producing at least a \n",
    "    # few thousand examples\n",
    "    for i in range(number_of_games):\n",
    "        env.reset()\n",
    "        # game_memory is new. It's described \n",
    "        # in the curriculum!\n",
    "        game_memory = []\n",
    "        prev_obs = []\n",
    "        score = 0\n",
    "        # We want a max number of game turns\n",
    "        # so that the game doesn't run forever\n",
    "        # on an bad set of inputs. only relevent\n",
    "        # to certain games however.\n",
    "        for turn in range(game_turns):\n",
    "            # we're just collecting data off of\n",
    "            # random agent played games. If \n",
    "            # gets to play 1000's of games, it\n",
    "            # will occassionally do well!\n",
    "            action = env.action_space.sample()\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            # summing the final score\n",
    "            score += int(reward)\n",
    "            # the first turn (or 0th turn) has no \n",
    "            # prev_obs, so skip it. otherwise,\n",
    "            # we tack 1) the previous observation\n",
    "            # and 2) the action taken during that \n",
    "            # state onto the game_memory\n",
    "            if turn > 0:\n",
    "                game_memory.append([prev_obs, int(action)])\n",
    "            # we cycle the obs so that on each\n",
    "            # step we have the previous obs \n",
    "            # stored when we recieve the new one\n",
    "            prev_obs = new_obs\n",
    "            # if the round finished, we want to\n",
    "            # break out of this for loop\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        # this occurs after each game completes. \n",
    "        # if the score from that game is above \n",
    "        # the threshold, we append that entire\n",
    "        # game onto X for training!\n",
    "        if score >= acceptable_score:\n",
    "            for data in game_memory:\n",
    "                X.append(np.array(data[0]).reshape(1, len(data[0])))\n",
    "                # the next two lines create our one hot\n",
    "                # labels array. We just set the index of\n",
    "                # our desired move to be a 1\n",
    "                predicted_action = list(one_hot)\n",
    "                predicted_action[data[1]] = 1\n",
    "                y.append(np.array(predicted_action).reshape(1, action_space))\n",
    "    print('{} examples were made.'.format(len(X)))\n",
    "    return np.array(X).reshape(-1, 1, len(data[0])), np.array(y).reshape(-1, 1, action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2986 examples were made.\n"
     ]
    }
   ],
   "source": [
    "X, y = initial_data(3000, 200, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word on validation_split. What this does is reserves a given percentage of the input data for validation testing. Basically, it's a way of keeping an eye on how well the training is doing while it's training. If the models ability to work with data outside of the training set is stagnant and not improving, the model is overfitting. This will be more useful in future projects. \n",
    "\n",
    "For now, this model only needs a single epoch to get good results. Obviously the accuracy and loss measures here don't exactly reflect how well the model is doing. 60% accuracy but aceing it every time? It's a bit more complicated than that. Definitely need to see the model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2388 samples, validate on 598 samples\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6736 - acc: 0.5942 - val_loss: 0.6679 - val_acc: 0.6154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13a6c44a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X, y=y, epochs=1, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(n_games, model=None):\n",
    "    for i in range(n_games):\n",
    "        env.reset()\n",
    "        prev_obs = []\n",
    "        score = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            # If no data is loaded up, take a random action\n",
    "            # If we're using a model, this will \n",
    "            # only happen on the 0th step\n",
    "            if (model == None) or (len(prev_obs) < 1):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # otherwise we use our model to choose an\n",
    "                # action based on the current observation (state)\n",
    "                action = np.argmax(model.predict(prev_obs.reshape(-1, 1, state_space)))\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            prev_obs = new_obs\n",
    "            score += reward\n",
    "                \n",
    "        env.close()\n",
    "        print('Final score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n",
      "Final score: 200.0\n"
     ]
    }
   ],
   "source": [
    "play_game(10, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
